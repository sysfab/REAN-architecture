{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# note:\n",
    "# if youre training on a server thru SSH, i would recommend you use the .py file\n",
    "# for training, as this would let you remount a TMUX terminmal if you disconnect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###   CONFIGURATION   ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from IPython.display import clear_output\n",
    "import tokenizer\n",
    "import os\n",
    "import math\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from datetime import datetime, timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word2vec\n",
    "model_file = fr\"./embedding_models/YOUR_GENSIM_MODEL_NAME.model\"\n",
    "embeddings_model = Word2Vec.load(model_file)\n",
    "\n",
    "vector_size = embeddings_model.vector_size        # aka embedding dim\n",
    "\n",
    "# neural net settings\n",
    "context_length = 128                              # tokens to consider\n",
    "attn_heads = 8                                    # num attention heads per mechanism (per transformer block)\n",
    "dropout_prob = 0.0                                # 0.0 ---> everything normal   |   1.0 ---> everything is random\n",
    "\n",
    "# dataset\n",
    "# !!!WARNING!!! bcs of various optimizations / errors on 8aafff's part small toy datasets dont work.\n",
    "# if ur running a mini dataset, copy paste the text inside multiple times for proper execution\n",
    "train_dataset_path = fr\"./datasets/YOUR_PLAINTEXT_TRAIN_DATASET.txt\"\n",
    "test_dataset_path = fr\"./datasets/YOUR_PLAINTEXT_TEST_DATASET.txt\"\n",
    "\n",
    "examples_train = 640#64 * 8 * 8 * 8 * 8 * 8 * 8 * 8\n",
    "examples_test = 640# * 8 * 8\n",
    "\n",
    "# train\n",
    "train_epochs = 120\n",
    "\n",
    "start_lr = 0.00003\n",
    "final_lr = 0.000001\n",
    "\n",
    "loss = nn.MSELoss()\n",
    "\n",
    "optimizer = torch.optim.Adam\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR\n",
    "\n",
    "train_batch_size = int(128)\n",
    "\n",
    "# eval\n",
    "eval_batch_size = int(128)\n",
    "eval_loop_batch = 64\n",
    "\n",
    "# test\n",
    "test_loop_batch = 256\n",
    "completion_length = 128\n",
    "test_prompts = [\"human: how do i cook poratoes and garlic? network: \",\n",
    "                \"human: what are some good circuit training excercises? network: \",\n",
    "                \"human: tell me about graphics cards and \",\n",
    "                \"network: as an ai language \",\n",
    "                \"\"]\n",
    "\n",
    "# pytorch\n",
    "run_device = torch.device(\"cuda\")\n",
    "storage_device = torch.device(\"cpu\")\n",
    "\n",
    "use_tensorboard = True\n",
    "log_dir = \"./runs\" # irrelevant if use_tensorboard = False\n",
    "run_name = \"exp1\" # irrelevant if use_tensorboard = False\n",
    "\n",
    "# checkpoints & backups\n",
    "save_checkpoint_batch = 512\n",
    "save_dir = log_dir + \"/\" + run_name + \"/\" + \"weights\"\n",
    "checkpoint_name = \"REAN_checkpoint_date_[DATE]_batch_[BATCH]_epoch_[EPOCH].pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# command to get freaky and bulldoze the entire server (needed in case FBI bust down the door):\n",
    "# sudo pkill \"python|ipython|ipykernel|tensor|tensorboard|board\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###   NEURAL NET ARCHITECTURE   ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class leaky_tanh_smart(nn.Module):\n",
    "    def __init__(self, leaky_range=(0, 3), squishy_range=(0, 3)):\n",
    "        super(leaky_tanh_smart, self).__init__()\n",
    "        # register leakyness and squishyness as trainable parameters\n",
    "        self.leakyness = nn.Parameter(torch.rand(1, dtype=torch.float32) * (leaky_range[1] - leaky_range[0]) + leaky_range[0])\n",
    "        self.squishyness = nn.Parameter(torch.rand(1, dtype=torch.float32) * (squishy_range[1] - squishy_range[0]) + squishy_range[0])\n",
    "        \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        applies the leaky tanh activation function over the input tensor x.\\n\n",
    "        for more info on leaky tanh and its parameters go to: https://www.desmos.com/calculator/kpzsfbtqww\n",
    "        \n",
    "        Args:\n",
    "            x (torch.Tensor): tensor over which to apply activation function.\n",
    "        \n",
    "        Returns:\n",
    "            torch.Tensor: returns x after function applied, keeps the same shape.\n",
    "        \"\"\"\n",
    "        \n",
    "        return F.tanh(x * self.squishyness) + self.leakyness * x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class attention_mech(nn.Module):\n",
    "    def __init__(self, vector_size=vector_size, attn_heads=attn_heads):\n",
    "        super(attention_mech, self).__init__()\n",
    "        # MultiheadAttention module\n",
    "        self.multihead_attn = nn.MultiheadAttention(embed_dim=vector_size, num_heads=attn_heads)\n",
    "        \n",
    "        # Layer normalization\n",
    "        self.norm = nn.LayerNorm(vector_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Prepare for multi-head attention (transpose to (sentence_len, batch_size, embedding_dim))\n",
    "        x = x.transpose(0, 1)\n",
    "        \n",
    "        # Create causal mask\n",
    "        seq_len = x.size(0)\n",
    "        causal_mask = torch.triu(torch.ones((seq_len, seq_len), device=x.device), diagonal=1).bool()\n",
    "        \n",
    "        # Apply multi-head attention with the causal mask\n",
    "        attn_output, attn_weights = self.multihead_attn(x, x, x, attn_mask=causal_mask)\n",
    "        \n",
    "        # Apply layer normalization to the attention output\n",
    "        attn_output = self.norm(attn_output)\n",
    "        \n",
    "        # Transpose back to (batch_size, sentence_len, embedding_dim)\n",
    "        output = attn_output.transpose(0, 1)\n",
    "        \n",
    "        return output, attn_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class positional_encoding(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(positional_encoding, self).__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, context_length, vector_size = x.size()\n",
    "\n",
    "        # Generate positions (shape: [context_length, 1])\n",
    "        position = torch.arange(0, context_length, dtype=torch.float).unsqueeze(1).to(x.device)\n",
    "\n",
    "        # Compute the divisor term (shape: [vector_size // 2])\n",
    "        div_term = torch.exp(torch.arange(0, vector_size, 2).float() * (-math.log(10000.0) / vector_size)).to(x.device)\n",
    "\n",
    "        # Initialize positional encoding tensor (shape: [context_length, vector_size])\n",
    "        pe = torch.zeros(context_length, vector_size, device=x.device)\n",
    "        \n",
    "        # Apply sine to even indices and cosine to odd indices\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)  # sine for even indices\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)  # cosine for odd indices\n",
    "\n",
    "        # Add positional encoding to the input\n",
    "        x = x + pe.unsqueeze(0)  # Add positional encoding, shape becomes (batch_size, context_length, vector_size)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class transformer_block(nn.Module):\n",
    "    def __init__(self, vector_size=vector_size):\n",
    "        super(transformer_block, self).__init__()\n",
    "        \n",
    "        self.activ_func = leaky_tanh_smart()\n",
    "        \n",
    "        self.attn = attention_mech()\n",
    "        \n",
    "        self.fc = nn.Linear(vector_size, vector_size)\n",
    "        \n",
    "        self.norm1 = nn.LayerNorm(vector_size)\n",
    "        self.norm2 = nn.LayerNorm(vector_size)\n",
    "        \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.norm1(x + self.attn(x)[0])\n",
    "        x = self.norm2(x + self.activ_func(self.fc(x)))\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class REAN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(REAN, self).__init__()\n",
    "        \n",
    "        self.pos_encoding = positional_encoding()\n",
    "        \n",
    "        self.tblock1 = transformer_block()\n",
    "        self.tblock2 = transformer_block()\n",
    "        self.tblock3 = transformer_block()\n",
    "        self.tblock4 = transformer_block()\n",
    "\n",
    "    def forward(self, segment: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        this function is primarily used for training, where the network needs to predict the next token, for every token in the sequence\n",
    "        \n",
    "        Args:\n",
    "            segment (torch.Tensor): this is a tensor of size (batches, context_length, vector_size) representing a sequence of tokens (of course from the tokenizer and using the correct word2vec model)\n",
    "        \n",
    "        Returns:\n",
    "            torch.Tensor: a tensor of shape (batches, context_length, vector_size) (same as segment) representing the sequence predicted by the network shifted future-way\n",
    "        \"\"\"\n",
    "        \n",
    "        ###                  INPUT                 ###\n",
    "        #    (batches, context_len, vector_size)\n",
    "        #                      ↓\n",
    "        \n",
    "        segment = self.pos_encoding(segment)\n",
    "        \n",
    "        segment = self.tblock1(segment)\n",
    "        segment = self.tblock2(segment)\n",
    "        segment = self.tblock3(segment)\n",
    "        segment = self.tblock4(segment)\n",
    "        \n",
    "        return segment\n",
    "    \n",
    "        #                      ↓\n",
    "        #    (batches, context_len, vector_size)\n",
    "        ###                 OUTPUT                 ###\n",
    "\n",
    "    def predict(self, segment: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        function is for predicting the embeddings vector of the next token in a given sequence\n",
    "        \n",
    "        Args:\n",
    "            segment (torch.Tensor): this is a tensor of size (batches, context_length, vector_size) representing a sequence of tokens (of course from the tokenizer and using the correct word2vec model)\n",
    "        \n",
    "        Returns:\n",
    "            torch.Tensor: a tensor of shape (batches, vector_size) representing the embeddings vector of the next token to be added into the sequence\n",
    "        \"\"\"\n",
    "        \n",
    "        ###                  INPUT                 ###\n",
    "        #    (batches, context_len, vector_size)\n",
    "        #                      ↓\n",
    "        \n",
    "        segment = self.forward(segment)\n",
    "        \n",
    "        return segment[:, -1, :]\n",
    "        \n",
    "        #                      ↓\n",
    "        #           (batches, vector_size)\n",
    "        ###                 OUTPUT                 ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###   BUILD NET & DEPENDENCIES   ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = REAN()\n",
    "\n",
    "net.to(run_device)\n",
    "\n",
    "optimizer = optimizer(net.parameters(), lr=start_lr)\n",
    "scheduler = scheduler(optimizer, T_max=train_epochs, eta_min=final_lr)\n",
    "\n",
    "print(f\"neural net weight: {sum(param.numel() * param.element_size() for param in net.parameters()) / (1024 ** 3):.4f}GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###   UTIL FUNCS   ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_segment(segment: list[str], model: Word2Vec=embeddings_model, default: int = 0, used_device=storage_device) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    encodes all words in a given list to corresponding vectors in given model.\n",
    "    words not found in the model will be given a vector with \"default\" value\n",
    "    \n",
    "    Args:\n",
    "        sentence (list): list of strings (tokenized sentence)\n",
    "        model (Word2Vec): model to use when encoding\n",
    "        default (int): fill vector with this value if word is not found in model\n",
    "    \n",
    "    Returns:\n",
    "        np.array: 2d array with dim1 = len(sentence) and dim2 = model.vector_size\n",
    "    \"\"\"\n",
    "    \n",
    "    # generate inital array with default values\n",
    "    vectorized = np.ones((len(segment), model.vector_size)) * default\n",
    "    \n",
    "    # loop over every word in list\n",
    "    for current_word, current_word_idx in zip(segment, range(len(segment))):\n",
    "        # only add correct values if word is in model, otherwise leave as default\n",
    "        if current_word in model.wv:\n",
    "            # the try except block is needed because (current_word in model.wv) sometimes gives a false positive... yeah gensim\n",
    "            try:\n",
    "                vectorized[current_word_idx] = model.wv.get_vector(current_word, norm=False)\n",
    "            except:\n",
    "                pass\n",
    "    \n",
    "    vectorized = torch.tensor(vectorized, dtype=torch.float32, device=used_device)\n",
    "    \n",
    "    return vectorized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def devectorize_segment(vectorized_segment: torch.Tensor, model: Word2Vec=embeddings_model, not_in_vocab_token=\"[NIV]\", NIV_threshold=0.01) -> list:\n",
    "    \"\"\"\n",
    "    decodes vectors into nearest word found in model, if no near words found, adds a not in vocab token\n",
    "    \n",
    "    Args:\n",
    "        vectorized_sentence (np.array): 2d arrat with vectors of words to be decoded\n",
    "        model (Word2Vec): model to use when decoding\n",
    "    \n",
    "    Returns:\n",
    "        list: list of strings (words) whos vectors most closely match those provided\n",
    "    \"\"\"\n",
    "    \n",
    "    result = []\n",
    "    \n",
    "    # make sure vectors are ready to be processed\n",
    "    vectorized_segment = vectorized_segment.cpu().numpy()\n",
    "    \n",
    "    # go over all words and find closest match in model\n",
    "    for current_word in vectorized_segment:\n",
    "        similarities = model.wv.similar_by_vector(current_word)\n",
    "        \n",
    "        # check if its not a bullshit vector\n",
    "        if similarities[0][1] > NIV_threshold:\n",
    "            result.append(similarities[0][0])\n",
    "        else:\n",
    "            result.append(not_in_vocab_token)\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_or_truncate(suspected_tensor: torch.tensor, target_length: int, default: int=0) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    pads or truncates a given tensor along dim 0 to target_length with \"default\" as padding\n",
    "    \n",
    "    Args:\n",
    "        suspected_tensor (torch.tensor): tensor to pad or truncate\n",
    "        target_length (int): target length of tensor\n",
    "        default (int): value to use for padding\n",
    "    \n",
    "    Returns:\n",
    "        torch.tensor: tensor of proper length no matter what\n",
    "    \"\"\"\n",
    "    \n",
    "    if len(suspected_tensor) < target_length:\n",
    "        # pad\n",
    "        suspected_tensor = torch.cat((torch.ones(target_length - len(suspected_tensor), suspected_tensor.shape[1], dtype=torch.float32, device=suspected_tensor.device) * default, suspected_tensor))\n",
    "    else:\n",
    "        # truncate\n",
    "        suspected_tensor = suspected_tensor[-target_length:]\n",
    "    \n",
    "    return suspected_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_segment_for_net(segment: list[str], length: int=context_length, used_device: torch.DeviceObjType=storage_device):\n",
    "    \"\"\"\n",
    "    function to take a sentence, and do everything to make it possible to input into the net\n",
    "    \n",
    "    Args:\n",
    "        segment (list[str]): a list of tokens (ideally from the tokenizer) of a sentence / text\n",
    "        length (int): the number of tokens to which pad or truncate to. for correct operation: keep at the net's context length\n",
    "    \n",
    "    Returns:\n",
    "        torch.Tensor: tokenized segment in the correct length\n",
    "    \"\"\"\n",
    "    \n",
    "    # turn into embedding vectors\n",
    "    vectorized = vectorize_segment(segment, used_device=used_device)\n",
    "    \n",
    "    # trim / add into length\n",
    "    trimmed = pad_or_truncate(vectorized, length)\n",
    "    \n",
    "    # add fake batch dimension\n",
    "    batched = trimmed.unsqueeze(0)\n",
    "    \n",
    "    return batched"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_word(segment: list[str], net: REAN=net):\n",
    "    # turn tokenized text into net's format\n",
    "    prepared_segment = prepare_segment_for_net(segment, used_device=next(net.parameters()).device)\n",
    "    \n",
    "    # run net\n",
    "    prediction_vector = net.predict(prepared_segment).detach()\n",
    "    \n",
    "    # turn vector back into token\n",
    "    predicted_token = devectorize_segment(prediction_vector)\n",
    "    \n",
    "    return predicted_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sequence(segment: list[str], num_tokens: int, net: REAN=net, display_tqdm=False):\n",
    "    result = segment.copy()\n",
    "    \n",
    "    for _ in tqdm(range(num_tokens), disable=not display_tqdm):\n",
    "        result += predict_word(result, net=net)\n",
    "    \n",
    "    return result[len(segment):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###   BUILD DATASET   ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class REAN_dataset(Dataset):\n",
    "    def pull_tokens(self, start_read_idx: int, requested_num_tokens: int):\n",
    "        \"\"\"\n",
    "        function returns a requested number of tokens from the dataset file, starting at APPROXIMATLY the start_read_idx token.\\n\n",
    "        attempts to return full words as much as possible, example:\\n\n",
    "        NO:    this | is | a | sen (tence)\\n\n",
    "        YES:   this | is | a | sentence\n",
    "        \n",
    "        Args:\n",
    "            start_read_idx (int): the APPROXIMATE token at which to start the reading (determined from the avarage token length in the tokenizer vocab)\n",
    "            requested_num_tokens (int): how many tokens to return\n",
    "        \n",
    "        Returns:\n",
    "            tokenized text (list of str): the tokens of the dataset from start_read_idx to start_read_idx + requested_num_tokens\n",
    "            is EOF hit (bool): if the requested args were outside of the dataset's range\n",
    "        \"\"\"\n",
    "        \n",
    "        with open(self.path, errors=\"ignore\") as self.dataset:\n",
    "            self.dataset.seek(start_read_idx * tokenizer.average_token_length)\n",
    "            \n",
    "            # get an initial estimate to what text we will actually need\n",
    "            self.buffer = self.dataset.read(requested_num_tokens * tokenizer.average_token_length)\n",
    "            self.tokenized_buffer = tokenizer.tokenize_segment(self.buffer)\n",
    "            self.current_num_tokens = len(self.tokenized_buffer)\n",
    "            \n",
    "            # if the estimate we took is too small, we enlarge it character by character until its perfect\n",
    "            while self.current_num_tokens < requested_num_tokens + 1:\n",
    "                self.next_char = self.dataset.read(1)  # seperate variable to check EOF\n",
    "                \n",
    "                # check eof\n",
    "                if not self.next_char:\n",
    "                    print(\"pull_tokens(): eof was hit\")\n",
    "                    return self.tokenized_buffer[-requested_num_tokens - 1:][:-1], True\n",
    "                \n",
    "                self.buffer += self.next_char\n",
    "                \n",
    "                self.tokenized_buffer = tokenizer.tokenize_segment(self.buffer)\n",
    "                self.current_num_tokens = len(self.tokenized_buffer)\n",
    "        \n",
    "        # regardless of if the estimate is too long / short, return theproper amount of tokens, with the end snipped of, because it might be a half token\n",
    "        return self.tokenized_buffer[-requested_num_tokens - 1:][:-1], False\n",
    "    \n",
    "    def construct_example(self, start_read_idx: int):\n",
    "        \"\"\"\n",
    "        function to make a full datapoint, can be used as raw return for __getitem__()\n",
    "        \n",
    "        Args:\n",
    "            start_read_idx (int): at which token to start making the example\n",
    "        \n",
    "        Returns:\n",
    "            tokenized text (list of str): the tokens of the dataset from start_read_idx to start_read_idx + self.context_length\n",
    "        \"\"\"\n",
    "        \n",
    "        # pull neccesary amount of tokens for question / input and answer / output\n",
    "        self.tokens, _ = self.pull_tokens(start_read_idx, self.context_length + 1)\n",
    "        \n",
    "        # encode the tokens to vectors (aka embeddings)\n",
    "        self.vectorized_tokens = prepare_segment_for_net(self.tokens, length=self.context_length + 1).squeeze(0)\n",
    "        \n",
    "        # split into network input and expected output\n",
    "        self.question = self.vectorized_tokens[:-1] # everythinbg up to last word\n",
    "        self.answer = self.vectorized_tokens[1:] # last word itself\n",
    "        \n",
    "        return self.question, self.answer\n",
    "    \n",
    "    def get_size(self):\n",
    "        \"\"\"\n",
    "        function to read thru the whole dataset, and report how many examples there are / if there are as many as the user requested\n",
    "        \n",
    "        Args:\n",
    "            none, but uses self.num_tokens and self.context_length\n",
    "        \n",
    "        Returns:\n",
    "            returns how many usable examples there are, for __len__()\n",
    "        \"\"\"\n",
    "        \n",
    "        with tqdm(total=self.num_examples, desc=\"Calculating Dataset Size\", unit=\"example\") as pbar:\n",
    "            for self.current_check in range(self.num_examples):\n",
    "                _, self.eof = self.pull_tokens(self.current_check, self.context_length)\n",
    "                \n",
    "                if self.eof:\n",
    "                    print(\"The requested size is bigger than the .txt provided, so the dataset might be smaller than what you expected.\")\n",
    "                    break\n",
    "\n",
    "                pbar.update(1)\n",
    "\n",
    "        print(f\"Requested num_examples: {self.num_examples}\\nActual size found:      {self.current_check - 1}\")\n",
    "        \n",
    "        return self.current_check - 1   # the -1 is just in case\n",
    "    \n",
    "    def __init__(self, path, num_examples, context_length, embeddings_model, verify_dataset_size=True):\n",
    "        # transfer to object wide variables\n",
    "        self.path = path\n",
    "        self.context_length = context_length\n",
    "        self.embeddings_model = embeddings_model\n",
    "        self.num_examples = num_examples\n",
    "        \n",
    "        # get the size of the dataset txt file\n",
    "        self.dataset_len = num_examples\n",
    "        \n",
    "        if verify_dataset_size:\n",
    "            self.dataset_len = self.get_size()\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.dataset_len\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.construct_example(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = REAN_dataset(train_dataset_path, examples_train, context_length, embeddings_model, verify_dataset_size=False)\n",
    "test_dataset = REAN_dataset(test_dataset_path, examples_test, context_length, embeddings_model, verify_dataset_size=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"please validate dataset: does this look correct?\\n\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    rnd_offset = random.randint(0, 100)\n",
    "    \n",
    "    for idx in range(3):\n",
    "        print(f\"sample {idx}:[nline]{tokenizer.detokenize_segment(devectorize_segment(train_dataset[idx + rnd_offset][0].detach(), embeddings_model))}[nline]------------------------------------------------------------[nline]{tokenizer.detokenize_segment(devectorize_segment(train_dataset[idx + rnd_offset][1].detach(), embeddings_model))}\".replace(\"\\n\", \" \").replace(\"[nline]\", \"\\n\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if num_workers arg is used\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=train_batch_size, shuffle=True)#, num_workers=4, persistent_workers=True)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=eval_batch_size, shuffle=True)#, num_workers=4, persistent_workers=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###   TRAIN   ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.train()\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_tensorboard:\n",
    "    writer = SummaryWriter(log_dir = log_dir + \"/\" + run_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = 0\n",
    "\n",
    "for epoch in range(train_epochs):\n",
    "    # training loop\n",
    "    for current_segment, target in train_loader:\n",
    "        batch += 1\n",
    "        \n",
    "        \n",
    "        # move batch to gpu\n",
    "        current_segment = current_segment.to(run_device)\n",
    "        target = target.to(run_device)\n",
    "        \n",
    "        # train batch\n",
    "        train_outputs = net(current_segment)\n",
    "        train_loss_value = loss(train_outputs, target)\n",
    "        train_loss_value.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        if use_tensorboard:\n",
    "            # Log training loss to TensorBoard\n",
    "            writer.add_scalar('train_loss', train_loss_value.item(), batch)\n",
    "        \n",
    "        # eval loop\n",
    "        if batch % eval_loop_batch == 0:\n",
    "            net.eval()\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                for test_current_segment, test_target in test_loader:\n",
    "                    # move batch to gpu\n",
    "                    test_current_segment = test_current_segment.to(run_device)\n",
    "                    test_target = test_target.to(run_device)\n",
    "                    \n",
    "                    # run test\n",
    "                    test_outputs = net(test_current_segment)\n",
    "                    test_loss_value = loss(test_outputs, test_target)\n",
    "                    \n",
    "                    if use_tensorboard:\n",
    "                        # Log test loss to TensorBoard\n",
    "                        writer.add_scalar('test_loss', test_loss_value.item(), batch)\n",
    "            \n",
    "            net.train()\n",
    "    \n",
    "        # test loop\n",
    "        if batch % test_loop_batch == 0:\n",
    "            if use_tensorboard:\n",
    "                for current_prompt in test_prompts:\n",
    "                    prediction = tokenizer.detokenize_segment(\n",
    "                        predict_sequence(tokenizer.tokenize_segment(current_prompt), completion_length)\n",
    "                    ).replace(\"\\n\", \"/n\")\n",
    "                    \n",
    "                    # Log predictions along with the prompt to TensorBoard with enhanced formatting\n",
    "                    formatted_text = (\n",
    "                        f\"---PROMPT---\\n{current_prompt}\"\n",
    "                        \"\\n\\n==========================================================================================================\\n\\n\"\n",
    "                        f\"---PREDICTION---\\n{prediction}\"\n",
    "                    )\n",
    "                    # Overwrite the previous entry for the same prompt\n",
    "                    writer.add_text(f'Predictions/{current_prompt}', formatted_text, batch)\n",
    "        \n",
    "        # save checkpoint\n",
    "        if batch % save_checkpoint_batch == 0:\n",
    "            os.makedirs(save_dir, exist_ok=True)\n",
    "            \n",
    "            torch.save(net, save_dir + \"/\" + checkpoint_name\n",
    "                       .replace(\"[DATE]\", (datetime.utcnow() + timedelta(hours=3)).strftime('%Y-%m-%d %H:%M:%S'))\n",
    "                       .replace(\"[BATCH]\", str(batch))\n",
    "                       .replace(\"[EPOCH]\", str(epoch)))\n",
    "    \n",
    "    # Update the learning rate scheduler\n",
    "    scheduler.step()\n",
    "    \n",
    "    if use_tensorboard:\n",
    "        # Log learning rate to TensorBoard\n",
    "        writer.add_scalar('learning_rate', optimizer.param_groups[0]['lr'], epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###   EVAL   ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.eval()\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"human: \" + \"write a list of the top 10 sports cars\" + \" network: \"\n",
    "tokens_to_predict = 128\n",
    "display_tqdm = True\n",
    "\n",
    "print(tokenizer.detokenize_segment(predict_sequence(tokenizer.tokenize_segment(prompt), tokens_to_predict, display_tqdm=display_tqdm)))\n",
    "\n",
    "# expected (in exact order):\n",
    "# 1. porche 2. bmw 3. mclaren 4. dodge 5. ferrari 6. mercedes etc...\n",
    "# if mercedes is put above BMW immediatly delete all traces of the .pth on ur machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CONDA_VENV",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
